\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsfonts,amsmath,amssymb,suetterl}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}

\usepackage{fontawesome}
\DeclareUnicodeCharacter{2212}{-}
\usepackage{mathrsfs}

\usepackage[nodisplayskipstretch]{setspace}

\pgfplotsset{compat = newest}
\setstretch{1.5}
\setlength{\headheight}{15.2pt}

\pagestyle{fancy}
\fancyfoot[C]{\thepage}
\fancyfoot[L]{Course Organizer: R. J. HARRIS}
\fancyfoot[R]{rosemary.harris@qmul.ac.uk}

% \renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0pt}
\parindent 0ex
\setlength{\parskip}{1em}

\begin{document}
  \textbf{MTH734U/MTHM012} \hfill \textbf{Mathematical Sciences}\\
  \textbf{Semester B 2010 - 2011} \hfill \textbf{QMUL}
  \begin{center}
    \textbf{\Huge Week 1}
  \end{center}
  \hrule \vspace{2mm} \hrule
  \section*{Key Objective:}
  \textit{Be familiar with the basic notions Of probability including: definition of a stochastic process, distributions of discrete and continuous random formula variables, calculation of expectation values, convolution formula for distribution of a sum}.
  \section*{Background:}
  Please use this week to recap basic probability concepts which you have covered in Probability II (or a equivalent course elsewhere). Use the lecture notes as a guide to what you need to know and read as necessary the relevant parts Of Chapters 1 and 2 of Taylor and Karlin, "An Introduction to Stochastic Modeling" [TA-Kl. Next week, amongst other things, we will utilize convolution formulae (cf. [T+KI Sec. 1.2.5) and the 'trick" with tail probabilities (see [T+KI Sec. 1.5.1). The 'Exercises" in [T+KI (answers at the back of the book) are short questions which provide a good diagnostic tool to check you understand things - I recommend you do a selection of those in Chapters 1 and 2 corresponding to the to topics you feel less confident about. The problems overleaf are of increasing difficulty; note that on these sheets indicates a particularly challenging (sub) question.
  \section*{Problems:}
  \begin{enumerate}
    \item \textbf{Random heating}\\
    Let $B$ be the the number of working boilers discrete in the Mathematics Building. Assume $B$ is a random variable having possible values 0, 1, and 2 and probability mass function
    \begin{align*}
      p(0) &= \frac{1}{10},\\
      p(1) &= \frac{3}{5},\\
      p(2) &= \frac{3}{10}.
    \end{align*}
    \begin{enumerate}
      \item Plot the corresponding distribution function.
      \item Determine the mean and variance of B.
    \end{enumerate}
    \item \textbf{Gamma distribution:}\\
    Consider a random variable $X$ with the probability density function
    $$
    f_X(x) = a^2xe^{-ax} \quad \text{for $x > 0$}
    $$
    where $a$ is a positive parameter. (This is a case of the Gamma distribution and will also be important later in the course.)
    \begin{enumerate}
      \item Sketch $f_X(x)$, Where is its maximum?
      \item Find the mean and variance of $X$.
      \item Calculate the Laplace transform
      $$
        \hat{F}_X(\lambda) = E[e^{-\lambda X}].
      $$
      \item Now consider a second random variable $Y$ which is drawn independently from the same distribution (i.e, $f_Y(y) = a^2ye^{-ay} \quad \text{for $y > 0$}$). Use convolution to find the probability density function of the sum $Z = X + Y$.
    \end{enumerate}
    \item \textbf{More practice with Laplace Transforms:}
    \begin{enumerate}
      \item Find $\hat{F}_X(\lambda)$ for the case $$dF_X(x) = e^{ax}dx.$$
      \item If $Y$ is uniformly distributed on $(0, 1)$, find $E[e^{-\lambda Y}]$.
      \item Let $Y_l,\ldots, Y_n$ be i.i.d. random variables each uniformly distributed on $(0, 1)$. What is the
      probability density function of $\sum_{i=1}^{n}Y_i$?
    \end{enumerate}
  \end{enumerate}
  %
  \newpage
  \textbf{MTH734U/MTHM012} \hfill \textbf{Mathematical Sciences}\\
  \textbf{Semester B 2010 - 2011} \hfill \textbf{QMUL}
  \begin{center}
    \textbf{\Huge Solutions for Week 1}
  \end{center}
  \hrule \vspace{2mm} \hrule
  \section*{General comments:}
  In several places in this problem sheet (and elsewhere) the following identity is very useful:
  \begin{equation}\tag{1.1}
    \lambda^{-n} = \int_0^\infty e^{-\lambda x}\frac{x^{n-1}}{(n-1)!}dx\quad \text{for $n \geq 1$}.
  \end{equation}
  You can check this straightforwardly using integration by parts and induction.
  \section*{Solutions to Problems:}
  \begin{enumerate}
    \item \textbf{Random heating}\\
    This was a nice gentle "warm-up" exercise.
    \begin{enumerate}
      \item The distribution function has the form:
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}
          \begin{axis}[
              axis lines = left,
              axis line style = {-latex, thick},
              xtick = {1,...,5},
              ytick = {1,...,10},
              xticklabel = \empty,
              yticklabel = \empty,
              xmax = 6,
              ymin = 0,
              ymax = 11,
              axis on top,
              clip = false
            ]
              %marks
              \addplot[color = black, mark = *, only marks, mark size = 2pt] coordinates {(0, 1) (1, 7) (2, 10)};
              %lines
              \addplot[color = black, thick] coordinates {(0, 1) (1, 1)};
              \addplot[color = black, thick] coordinates {(1, 7) (2, 7)};
              \addplot[color = black, thick] coordinates {(2, 10) (5, 10)};
              %labels
              \node [below right] at (6, 0){$b$};
              \node [above left] at (0, 11) {$F_B(b)$};
              \node [below] at (0, -0.15) {$0$};
              \node [below] at (1, -0.15) {$1$};
              \node [below] at (2, -0.15) {$2$};
              \node [left] at (-0.15, 1) {$\frac{1}{10}$};
              \node [left] at (-0.15, 7) {$\frac{7}{10}$};
              \node [left] at (-0.15, 10) {$1$};
            \end{axis}
          \end{tikzpicture}
        \end{figure}
      \item The mean is given by
      \begin{align*}
        E[B]
        &= 0\times \frac{1}{10} + 1\times\frac{3}{5} + 2\times\frac{3}{10}\\
        &= \frac{6}{5},
      \end{align*}
      while the variance is
      \begin{align*}
        \text{Var}[B]
        &= E[B^2] - (E[B])^2\\
        &= 0^2\times\frac{1}{10} + 1^2\times\frac{3}{5} + 2^2\times\frac{3}{10} - \left(\frac{6}{5}\right)^2\\
        &= \frac{9}{25}.
      \end{align*}
    \end{enumerate}
    \item \textbf{Gamma distribution}
    \begin{enumerate}
      \item One can show by differentiation that the maximum is at $x = l/a$ so a sketch graph looks like:
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}
          \begin{axis}[
              xmin = 0, xmax = 2,
              ymin = 0, ymax = 2,
              axis lines = left,
              xtick = \empty,
              ytick = \empty,
              xticklabel = \empty,
              yticklabel = \empty,
              axis on top,
              clip = false
            ]
            % Plot a function
            \addplot[
              domain = 0:2,
              samples = 200,
              smooth,
              thick,
            ] {16*x*exp(-4*x)};
            \addplot[color = black, thick] coordinates {(0.25, 0.05) (0.25, -0.05)};
            \addplot[color = black, thick] coordinates {(0.05, 1.47) (-0.05, 1.47)};
            %label
            \node [below right] at (2, 0){$x$};
            \node [above left] at (0, 2) {$f_X(x)$};
            \node [below] at (0.25, -0.05){$\frac{1}{a}$};
            \node [left] at (-0.05, 1.47){$\frac{a}{e}$};
          \end{axis}
        \end{tikzpicture}
      \end{figure}
      \item The mean is given by
      \begin{align*}
        \mu
        &= E[X]\\
        &= \int_0^\infty xf_X(x)dx\\
        &= \int_0^\infty a^2x^2e^{-ax}dx\\
        &= \frac{2}{a}.
      \end{align*}
      [To do the integral either integrate by parts or use (1.1) with $n = 3$]\\
      A similar integration yields
      \begin{align*}
        E[X^2]
        &= \int_0^\infty x^2f_X(x)dx\\
        &= \int_0^\infty a^2x^3e^{-ax}dx\\
        &= \frac{6}{a^2},
      \end{align*}
      and hence the variance
      \begin{align*}
        \sigma^2
        &= E[X^2] - \mu^2\\
        &= \frac{6}{a^2}-\left(\frac{2}{a}\right)\\
        &= \frac{2}{a^2}.
      \end{align*}
      Note that the question means "find" in the sense of calculate, not merely in the sense of look-up somewhere! Of course, one can also obtain the moments by calculating the moment generating function but that is not actually necessary for this part of the question, and is essentially equivalent to calculating the Laplace transform in (c). [Why?]
      \item The Laplace transform is
      \begin{equation}\tag{1.2}
        \begin{aligned}[b]
          \hat{F}_X(\lambda)
          &= E[e^{-\lambda X}]\\
          &= \int_0^\infty e^{-\lambda x}f_X(x)dx\\
          &= \int_0^\infty a^2xe^{-(a + \lambda)x}dx\\
          &= \frac{a^2}{(a + \lambda)^2}.
        \end{aligned}
      \end{equation}
      [TO do the integral use parts or (1.1) again, or just observe that $\int_0^\infty(a + \lambda)^2\allowbreak xe^{-(a + \lambda)x}dx = 1$ since it is the integral over a proper probability distribution function.]
      \item Since $X$ and $Y$ are independent continuous random variables, the p.d.f. of their sum $Z$ is given by
      \begin{equation}\tag{1.3}
        \begin{aligned}[b]
          f_Z(z)
          &= \int_0^\infty f_X(z - y)f_Y(y)dy\\
          &= \int_0^\infty a^2(z - y)e^{-a(z - y)}a^2ye^{-ay}dy\\
          &= a^4e^{-az}\int_0^z(z - y)ydy\\
          &= a^4e^{-az}\left[z\frac{y^2}{2} - \frac{y^3}{3}\right]_{y = 0}^{y = z}\\
          &= \frac{a^4z^3e^{-az}}{6}.
        \end{aligned}
      \end{equation}
      An alternative (and arguably nicer) route for non-negative random variables is to use Laplace transforms. Recall that the Laplace transform of a convolution is just the product of the individual transforms. Hence, using (1.2) we have:
      \begin{align*}
        \hat{F}_Z(\lambda)
        &= \left(\frac{a^2}{(a + \lambda)^2}\right)^2\\
        &= \frac{a^4}{(a + \lambda)^4}.
      \end{align*}
      Now the only difficulty is doing the inverse Laplace transform. Once again (1.1) comes to our rescue. We can use this to write
      \begin{align*}
        \frac{a^4}{(a + \lambda)^4} 
        &= a^4\int_0^\infty e^{-(a + \lambda)z}\frac{z^3}{3!}dz\\
        &= \int_0^\infty e^{-\lambda z}\left(\frac{a^4z^3e^{-az}}{6}\right)dz.
      \end{align*}
      The Laplace transform uniquely determines the distribution, hence we must have
      $$
      f_Z(z) = \frac{a^4z^3e^{-ax}}{6},
      $$
      in agreement with (1.3).
    \end{enumerate}
    \item \textbf{Practice with Laplace Transforms}
    \begin{enumerate}
      \item First recall that $dF_X(x) = e^{ax}dx$ simply implies a density function of $f_X(x) = e^{ax}$. Note that the question should have explicitly stated that $x\ geq 0$. Note also that, as it stands, this is not a proper probability distribution since it is not normalized (and indeed it is only normalizable if a is negative)' Leaving aside questions of interpretation, the integral involved in the Laplace transform is straightforward:
      $$
      \hat{F}_X(\lambda) = \int_0^\infty e^{-\lambda x}e^{ax}dx =
      \begin{cases}
        (\lambda - a)^{-1} & \text{if $\lambda > a$}\\
        \infty & \text{if $\lambda \leq a$}.
      \end{cases}
      $$
      For $X \leq a$, we say that the Laplace transform does not exist.
      \item For $Y$ uniformly distributed over $(0, 1)$ we obviously have
      \begin{equation}\tag{1.4}
        f_Y(y) =
        \begin{cases}
          1 & \text{if $0 < y < 1$}\\
          0 & \text{otherwise}
        \end{cases}
      \end{equation}
      Hence
      \begin{align*}
        E[e^{-\lambda Y}]
        &= \int_0^\infty e^{-\lambda y}dy\\
        &= \left[-\frac{e^{-\lambda y}}{\lambda}\right]^{y = 1}_{y = 0}\\
        &= \frac{(1 - e^{-\lambda})}{\lambda}
      \end{align*}
      \item Let us denote the common distribution function of $Y_1, \ldots, Y_n$ by $F$ and the distribution
      function of their sum by $F_n$. Now
      \begin{align*}
        E\left[e^{-\lambda\sum_{i=1}^nY_i}\right]
        &= E\left[\prod_{i = 1}^ne^{-\lambda Y_i}\right]\\
        &= \left(E[e^{-\lambda Y_1}]\right)^n \quad\text{[by independence]}
      \end{align*}
      or in other words,
      $$
      \hat{F}_n(\lambda) = (\hat{F}(\lambda))^n.
      $$
      (Again this is just the statement that the Laplace transform of a convolution is the product of the Laplace transforms.) So if F is the uniform distribution of part (b) we have
      \begin{equation}\tag{1.5}
        \hat{F}_n(\lambda) = \left(\frac{(1 - e^{-\lambda})}{\lambda}\right)^n.
      \end{equation}
      As usual the hard bit is doing the inverse Laplace transform to get the density $f_n$. The first step is to use a binomial expansion to write (1.5) as
      $$
      \hat{F}_n(\lambda) = \sum_{k = 0}^n\binom{n}{k}(-1)^ke^{-\lambda k}\lambda^{-n}.
      $$
      So we need the inverse transform of $e^{-\lambda k}\lambda^{-n}$. The trick is to identify the inverse transforms of the separate factors:
      \begin{itemize}
        \item $e^{-\lambda k}$ is the Laplace transform Of a Dirac delta distribution, i.e.,
        $$
        e^{-\lambda k} = \int_0^\infty e^{-\lambda y}\delta(y - k)dy,
        $$
        where $\delta(y - k)$ is defined such that
        $$
        \int_{-\infty}^\infty g(y)\delta(y - k)dy = g(k)
        $$
        for any continuous function $g$.
        \item $\lambda^{-n}$ is the Laplace transform of $y^{n - 1}/(n - 1)!, y \geq 0$ [from (1.1)].
      \end{itemize}
      Combining these facts, we see that $e^{-\lambda k}\lambda^{-n}$ is the transform of the convolution
      $$
      \int_0^\infty \frac{(y - z)^{n - 1}}{(n - )!}\delta(z - k)dz =
      \begin{cases}
        0 & \text{if $u < k$}\\
        \frac{(y - k)^{n - 1}}{(n - 1)!} & \text{if $y\geq k$}.
      \end{cases}
      $$
      Hence, using the notation $(A)_+$ for the positive part of $A$, the required density is
      \begin{equation}\tag{1.6}
        f_n(y) = \sum_{k = 0}^n\binom{n}{k}(-1)^k\frac{(y - k)^{n - 1}_+}{(n - 1)!}
      \end{equation}
      Note that this is the p.d.f. of the so-called C'Irwin-Hall distribution" and can also be written as
      $$
      f_n(y) = \frac{1}{2}\sum_{k = 0}^n\binom{n}{k}(-1)^k\frac{(y - k)^{n - 1}sgn(y - k)}{(n - 1)!}
      $$
      where
      $$
      sgn(y - k) =
      \begin{cases}
        -1 & \text{for $x < k$}\\
        0 & \text{for $x = k$}\\
        1 & \text{for $x> k$}
      \end{cases}.
      $$
      After all this work, it would be good to check that, for $n = 1$, (1.6) reduces back to (1.4). This is left as an exercise for the reader.
    \end{enumerate}
  \end{enumerate}
\end{document}