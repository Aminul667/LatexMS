\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsfonts,amsmath,amssymb,suetterl}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage[utf8]{inputenc}

\usepackage{fontawesome}
\DeclareUnicodeCharacter{2212}{-}
\usepackage{mathrsfs}

\usepackage[nodisplayskipstretch]{setspace}

\setstretch{1.5}
\setlength{\headheight}{15.2pt}

\pagestyle{fancy}
\fancyfoot[C]{\thepage}
\fancyfoot[L]{Course Organizer: R. J. HARRIS}
\fancyfoot[R]{rosemary.harris@qmul.ac.uk}

% \renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0pt}
\parindent 0ex
\setlength{\parskip}{1em}

\begin{document}
  \textbf{MTH734U/MTHM012} \hfill \textbf{Mathematical Sciences}\\
  \textbf{Semester B 2010 - 2011} \hfill \textbf{QMUL}
  \begin{center}
    \textbf{\huge Week 2}
  \end{center}
  \hrule \vspace{2mm} \hrule
  %
  \section*{Key Objective:}
  \textit{Know the definition of a \textbf{renewal process} and the associated random variables. Be able to derive the expression for the renewal function in terms of the distribution of interoccurrence times; perform explicit calculations for simple cases such as (i) the Poisson process and (ii) a gamma density of interccurrence times.}
  %
  \section*{Background:}
  Please read [T + K] Secs. VII.1 - VII.3. The formalism I introduced in the lecture can be found in Sec. VII.I so make sure you go over all the details and understand the relationships between the various random variables associated with a renewal process. Sec. VII.2 contains a number of applications and examples but, for the moment, you can skip the block replacement material in 2.2 since we will consider discrete renewal theory in much more detail later.\par
  It is usually hard to calculate the renewal function explicitly although, as we will see next week, one can make general statements about the asymptotic $t \to \infty$ behaviour. For continuous lifetimes there are two important cases where the renewal function can be relatively straightforwardly obtained: the Poisson process (see [T + K] Sec. V11.3) and a gamma density of interoccurence times. Both these cases feature in the problems overleaf.
  \newpage
  \section*{Problems:}
  \begin{enumerate}
    \item \textbf{Poisson process}\\
    Consider a renewal process in which the interoccurrence times have the exponential distribution with parameter $a > 0$, i.e.,
    $$
    f(x) = \alpha e^{-\alpha x},\quad \text{and $F(x) = 1 - e^{-\alpha x}$ for $x > 0$}.
    $$
    Find the renewal function $M(t) = E[N(t)]$ by each of the following methods.
    \begin{enumerate}
      \item Identify this as the Poisson process and use the known distribution of $N(t)$.
      \item Show directly by convolution that
      $$
      f_n(x) = \frac{\alpha^nx^{n - 1}e^{-\alpha x}}{(n - 1)!}\quad \text{for $x > 0$}.
      $$
      [Hint: Use induction.] Now write
      \begin{align*}
        M(t)
        &= \sum_{n = 1}^\infty F_n(t)\\
        &= \sum_{n = 1}^\infty\int_0^tf(x)dx
      \end{align*}
      and interchange the order of integral and sum to facilitate evaluation.
      \item Find the Laplace transform
      $$
      \hat{F}(\lambda):=\int_0^\infty f(x)e^{-\lambda x}dx
      $$
      and use this to obtain $\hat{F}_n(\lambda)$. Now you can sum over $n$ and then do the inverse Laplace transform to get an expression for $\sum_{n = 0}^\infty f_n(x)$ which leads to $M(t)$.
    \end{enumerate}
    \item \textbf{Mean excess life}\\
    Prove that the mean excess life can be evaluated in terms of the renewal function via the relation
    $$
    E[\gamma_t] = E[X_1]\{1 + M(t)\} - t.
    $$
    Verify this relationship explicitly for the Poisson process.
    \item \textbf{Gamma density}\\
    Now consider a renewal process in which the interoccurrence times have the gamma density:
    $$
    f(x) = xe^{-x}\quad\text{for $x \geq 0$}.
    $$
    Find the renewal function using either of the methods used in parts (b) or (c) of Question 1.
  \end{enumerate}
  %
  \newpage
  \textbf{MTH734U/MTHM012} \hfill \textbf{Mathematical Sciences}\\
  \textbf{Semester B 2010 - 2011} \hfill \textbf{QMUL}
  \begin{center}
    \textbf{\huge Solutions for Week 2}
  \end{center}
  \hrule \vspace{2mm} \hrule
  %
  \section*{General comments:}
  For background material to this problem sheet see secs. V11.1-V11.3 of [T+KI. In particular sec. V11.3 covers the interpretation of a Poission process as a renewal process with an exponential distribution of interoccurence times. Note that the book does not use Laplace transforms which in many cases provide an elegant alternative method of solution (see below).
  \section*{Solutions to Problems:}
  \begin{enumerate}
    \item \textbf{Solutions to Problems:}
    \begin{enumerate}
      \item The given distribution of interoccurence times defines a Poisson process with parameter $\alpha$. Hence the number of renewal events in time t has the Poisson distribution
      $$
      \text{Pr}\{N(t) = k\} = \frac{(\alpha t)^ke^{-\alpha t}}{k!},\quad k = 0, 1,\ldots
      $$
      and
      \begin{equation}\tag{2.1}
        \begin{aligned}[b]
          M(t)
          &= E[N(t)]\\
          &= \sum_{k = 1}^\infty k\text{Pr}\{N(t) = k\}\quad \text{[$k = 0$ term doesn't contribute]}\\
          &= \sum_{k = 1}^\infty \frac{(\alpha t)^ke^{-\aleph t}}{(k - 1)!}\\
          &= \alpha t\sum_{k = 1}^\infty \frac{(\alpha t)^{k-1}e^{-\alpha t}}{(k - 1)!}\\
          &= \alpha t\sum_{m = 0}^{\infty}\frac{(\alpha t)^me^{-\alpha t}}{m!}\quad \text{[Change variables $m = k - 1$]}\\
          &= \alpha t.
        \end{aligned}
      \end{equation}
      \item Proposition: The $n$-fold convolution of $f$ is given by
      \begin{equation}\tag{2.2}
        f_n(x) = \frac{\alpha^nx^{n - 1}e^{-\alpha x}}{(n - 1)!}\quad \text{for $x$ > 0},
      \end{equation}
      Proof: Use induction as follows.
      \begin{itemize}
        \item Base case is $n = 1$
        \begin{align*}
          f_1(x) 
          &= \alpha e^{-\alpha x}\quad \text{for $x > 0$}\\
          &= f(x).
        \end{align*}
        \item Assume Proposition is true for $n = k$. Then, for $x > 0$,
        \begin{align*}
          f_{k + 1}(x)
          &= \int_0^x f_k(x - y)f(y)dy\quad \text{[definition of convolution]}\\
          &= \int_0^\infty \frac{\alpha^k(x - y)^{k - 1}e^{-\alpha(x - y)}}{(k - 1)!}\alpha e^{-\alpha y}dy\quad \text{[by induction hypothesis]}\\
          &= \alpha^{k + 1}e^{-\alpha x}\int_0^\infty\frac{(x - y)^{k - 1}}{(k - 1)!}dy\\
          &= \alpha^{k + 1}e^{-\alpha x}\left[\frac{-(x - y)^k}{k!}\right]_{y = 0}^{y = x}\\
          &= \frac{\alpha^{k + 1}x^ke^{-\alpha x}}{k!}.
        \end{align*}
        So, if the proposition is true for $n = k$, it is also true for $n = k + 1$.
        \item Hence, by induction, the proposition is true for all integer $n \geq 1$.
      \end{itemize}
      Then, as instructed, use
      \begin{equation}\tag{2.3}
        \begin{aligned}[b]
          M(t)
          &= \sum_{n = 1}^\infty F_n(t)\\
          &= \sum_{n = 1}^\infty\int_0^t f_n(x)dx\\
          &= \int_0^t\left(\sum_{n = 1}^\infty f_n(x)\right)dx\\
          &= \int_0^t\left(\sum_{n = 1}^\infty \frac{\alpha^nx^{n - 1}e^{-\alpha x}}{(n - 1)!}\right)dx\\
          &= \int_0^t\left(\alpha\sum_{n = 1}^\infty\frac{(\alpha x)^{n - 1}e^{-\alpha x}}{(n - 1)!}\right)dx\\
          &= \int_0^t\alpha dx\\
          &= \alpha t.
        \end{aligned}
      \end{equation}
      \item Laplace transform:
      \begin{align*}
        \hat{F}(\lambda)
        &= \int_0^\infty f(x)e^{-\lambda x}dx\\
        &= \int_0^\infty \alpha e^{-\alpha x}e^{-\lambda x}dx\\
        &= \int_0^\infty \alpha e^{-(\alpha + \lambda)x}dx\\
        &= \left[\frac{-\alpha}{\alpha + \lambda}e^{-(\alpha + \lambda)x}\right]_{x = 0}^{x = \infty}\\
        &= \frac{\alpha}{\alpha + \lambda}
      \end{align*}
      [Strictly speaking, the integral only converges for $\alpha + \lambda > 0$ but it's not really necessary to write that here since $\lambda \geq 0$ is usually assumed for Laplace transforms and $\alpha > 0$ is given in the question.] Now, since the Laplace transform of a convolution is the product of the transforms, we have
      \begin{equation}\tag{2.4}
        \begin{aligned}[b]
          \hat{F}_n(\lambda) 
          &= (\hat{F}(\lambda))^n\\
          &= \left(\frac{\alpha}{(\alpha + \lambda)}\right)
        \end{aligned}
      \end{equation}
      and hence
      \begin{align*}
        \sum_{n = 1}^\infty\hat{F}_n(\lambda)
        &= \sum_{n = 1}^\infty \left(\frac{\alpha}{\alpha + \lambda}\right)^n\\
        &= \frac{\alpha}{\alpha + \lambda}\sum_{n = 0}^\infty\left(\frac{\alpha}{\alpha + \lambda}\right)^n\\
        &= \frac{\alpha}{\alpha + \lambda}\times \frac{1}{1 - \left(\frac{\alpha}{\alpha + \lambda}\right)}\\
        &= \frac{\alpha}{\lambda}.
      \end{align*}
      To do the inverse Laplace transform note that (cf. solutions to the Week 1 problem set)
      \begin{equation}\tag{2.5}
        \frac{\alpha}{\lambda} = \int_0^\infty e^{-\lambda x}\alpha dx
      \end{equation}
      The Laplace transform uniquely determines the distribution so
      $$
      \sum_{n = 1}^\infty f_n(x) = \alpha
      $$
      and
      \begin{equation}\tag{2.6}
        \begin{aligned}[b]
          M(t)
          &= \int_0^\infty\left(\sum_{n = 1}^\infty f_n(x)\right)dx\\
          &= \int_0^\infty \alpha dx\\
          &= \alpha t.
        \end{aligned}
      \end{equation}
      Note the happy agreement between results (2.1), (2.3) and (2.6).
    \end{enumerate}
    \item \textbf{Mean excess life}\\
    First you need to prove
    $$
    E[W_{N(t) + 1}] = E[X_1]\{M(t) + 1\}.
    $$
    This is a standard derivation which was sketched in the lecture and can be found in full on pages 423-424 of $[T+K]$. \textit{You should be able to reproduce the steps}. (Note in particular the trick with indicator variables allowing the replacement of $N(t) + 1$ with $\infty$ in the upper limit of the sum.)\par
    After this, it's easy. The excess life is defined as
    $$
    \gamma_t = W_{N(t) + 1} - t,
    $$
    so
    \begin{equation}\tag{2.7}
      \begin{aligned}[b]
        E[\gamma_t]
        &= E[W_{N(t) + 1}] - t\\
        &= E[X_1]\{M(t) + 1\} - t
      \end{aligned}
    \end{equation}
    For a Poisson process with parameter $\alpha$
    \begin{align*}
      \text{Pr}\{\gamma_t > x\}
      &= \text{Pr}\{N(t + x) - N(t) = 0\}\quad \text{[probability of no renewals in interval $(t, t + x)$}\\
      &= \text{Pr}\{N(x) - N(0) = 0\}\quad \text{[stationary independent increments]}\\
      &= e^{-\alpha x}.
    \end{align*}
    So the mean excess life is given by
    \begin{equation}\tag{2.8}
      \begin{aligned}[b]
        E[\gamma_t]
        &= \int_0^\infty \text{Pr}\{\gamma_t > x\}dx\\
        &= \int_0^\infty e^{-\alpha x}dx\\
        &= \left[\frac{-e^{-\alpha x}}{\alpha}\right]_{x = 0}^{x = \infty}\\
        &= \frac{1}{\alpha}.
      \end{aligned}
    \end{equation}
    The fact that the excess life has the same distribution as the interoccurence times $X_k$ (and hence the same mean) is a statement of the memoryless property of the exponential distribution. With (2.8) and (2.1) to hand, the relation (2.7) is straightforwardly verified.
    \item \textbf{Gamma density}\\
    First let's do it without Laplace transforms, i.e., using the method of 1(b). We have
    \begin{equation}\tag{2.9}
      f_1(x) = f(x) = xe^{-x}\quad \text{for $x > 0$}
    \end{equation}
    and, from Problem 2 on last week's sheet (with $\alpha = 1$),
    \begin{equation}\tag{2.10}
      f_2(x) = \frac{x^3e^{-x}}{6}\quad \text{for $x > 0$}.
    \end{equation}
    We want to know $f_n(x)$. Inspection of (2.9) and (2.10) leads us to the proposition
    \begin{equation}\tag{2.11}
      f_n(x) = \frac{x^{2n - 1}e^{-x}}{(2n - 1)}\quad \text{for $x > 0$}
    \end{equation}
    which we prove by induction:
    \begin{itemize}
      \item Base case $n = 1$ given by (2.9).
      \item Assume proposition is true for $n = k$. Then, for $x > 0$,
      \begin{align*}
        f_{k + 1}(x)
        &= \int_0^xf_k(x - y)f(y)dy\\
        &= \int_0^x\frac{(x - y)^{2k - 1}e^{-(x - y)}}{(2k - 1)!}ye^{-y}dy\quad \text{[by induction hypothesis]}\\
        &= \frac{e^{-x}}{(2k - 1)!}\int_0^x(x - y)^{2k - 1}ydy\\
        &= \frac{e^{-x}}{(2k - 1)}\int_0^xu^{2k - 1}(x - u)du\quad \text{[change variables $u = x - y$]}\\
        &= \frac{e^{-x}}{(2k - 1)!}\left[\frac{xu^{2k}}{2k} - \frac{u^{2k + 1}}{2k + 1}\right]_{u = 0}^{u = x}\\
        &= \frac{x^{2k + 1}e^{-x}}{(2k - 1)!}\left(\frac{1}{2k} - \frac{1}{2k + 1}\right)\\
        &= \frac{x^{2k + 1}e^{-x}}{(2k + 1)!}
      \end{align*}
      So, if the proposition is true for $n = k$, it is also true for $n = k + 1$.
      \item Hence, by induction, the proposition is true for all integer $n \geq 1$.
    \end{itemize}
    Actually there is a quicker way to see this. Note that $f(x)$ of (2.9) is just the density of the sum of two independent exponentially-distributed random variables with parameter $\alpha = 1$ (cf. (2.2) with $n = 2$ and $\alpha = 1$). Hence, the required density of the sum of $n$ variables with this gamma distribution, is the same as the density of the sum of $2n$ exponentially-distributed variables. In other words we can get (2.11) from (2.2) by setting $\alpha = 1$ and replacing $n$ by $2n$.\par
    Now we proceed as before:
    \begin{equation}\tag{2.12}
      \begin{aligned}[b]
        M(t)
        &= \sum_{n = 1}^\infty \int_0^tf_n(x)dx\\
        &= \int_0^t \left(\sum_{n = 1}^\infty f_n(x)\right)dx\\
        &= \int_0^t \left(e^{-x}\sum_{n = 1}^\infty \frac{x^{2n - 1}}{(2n - 1)!}\right)dx\\
        &= \int_0^t\left(e^{-x}\frac{e^x - e^{-x}}{2}\right)dx\quad \text{[recognizing sum as series for $\sinh(x)$]}\\
        &= \frac{1}{2}\int_0^t(1 - e^{-2x})dx\\
        &= \frac{1}{2}\left[x + \frac{e^{-2x}}{2}\right]_{x = 0}^{x = t}\\
        &= \frac{t}{2} - \frac{1}{4}(1 - e^{-2t}). 
      \end{aligned}
    \end{equation}
    Of course we can get the same result using Laplace transforms. From Problem 2 on last week's problem sheet, we have
    $$
    \hat{F}(\lambda) = \frac{1}{(1 + \lambda)^2}
    $$
    and therefore
    $$
    \hat{F}_n(\lambda) = \frac{1}{(1 + \lambda)^{2n}}.
    $$
    [This result can also be obtained from (2.4) by setting $\alpha = 1$ and replacing $n$ by $2n$.] Again we sum over $n$ before doing the inverse Laplace transform:
    \begin{align*}
      \sum_{n = 1}^\infty \hat{F}_n(\lambda)
      &= \sum_{n = 1}^\infty\left(\frac{1}{(1 + \lambda)^2}\right)^n\\
      &= \frac{1}{(1 + \lambda)^2}\sum_{n = 0}^\infty\left(\frac{1}{(1 + \lambda)^2}\right)^n\\
      &= \frac{1}{(1 + \lambda)^2}\times \frac{1}{1 - \left(\frac{1}{(1 + \lambda)^2}\right)}\\
      &= \frac{1}{\lambda^2 + 2\lambda}\\
      &= \frac{1}{2\lambda} - \frac{1}{2(\lambda + 2)}\quad\text{[partial fraction]}\\
      &= \int_0^\infty e^{-\lambda x}\frac{1}{2}dx - \int_0^\infty e^{-(\lambda + 2)x}\frac{1}{2}dx\quad \text{[cf. (2.5)]}\\
      &= \int_0^\infty e^{-\lambda x}\left(\frac{1}{2}(1 - e^{-2x})\right)dx.
    \end{align*}
    Hence
    $$
    \sum_{n = 0}^\infty f_n(x) = \frac{1}{2}(1 - e^{-2x}),
    $$
    and
    \begin{align*}
      M(t)
      &= \int_0^\infty\left(\sum_{n = 1}^\infty f_n(x)\right)dx\\
      &= \int_0^\infty \frac{1}{2}(1 - e^{-2x})dx\\
      &= \frac{t}{2} - \frac{1}{4}(1 - e^{-2t}),
    \end{align*}
    in agreement with (2.12)
  \end{enumerate}
\end{document}